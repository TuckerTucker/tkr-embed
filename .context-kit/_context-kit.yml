# Project configuration for AI agents - tkr-embed
# tkr-embed | GPT-OSS-20B Text Generation Server with Context Kit monitoring - Complete Transformation
# Generated from text generation transformation analysis - optimized for token efficiency
meta:
  kit: tkr-embed-gpt
  fmt: 9
  type: mlx-text-generation-server
  desc: "TRANSFORMED: Apple Silicon GPT-OSS-20B text generation server with reasoning levels and Context Kit monitoring"
  ver: "2.0.0"
  author: "Tucker github.com/tuckertucker"
  ts: "2025-09-13"
  status: transformation-complete
  entry: "source start_env"
  stack: &tech-stack "Python + MLX 0.29.0 + GPT-OSS-20B + FastAPI + Context Kit + Apple Silicon M1 + Text Generation"
  model: "openai/gpt-oss-20b (21B parameters, auto-quantization)"
  platform: "Apple Silicon M1 32GB with Metal GPU optimization"
  cmds: ["source start_env", "python -m tkr_embed.api.server", "cd .context-kit/dashboard && npm run dev"]

# Dependencies with Context7 references - TRANSFORMED for text generation
deps: &deps
  # TRANSFORMED: Python ML/AI stack for text generation server
  py: &py-deps
    prod:
      # âœ… MLX ecosystem for text generation
      mlx: &mlx-dep {id: "/ml-explore/mlx", v: "0.29.0", status: "âœ… INSTALLED"}
      mlx-lm: {id: "/ml-explore/mlx-lm", v: ">=0.22.0", status: "âœ… INSTALLED"}

      # âœ… ML and model hosting - 21B model support
      huggingface-hub: {id: "/huggingface/huggingface_hub", v: ">=0.27.0", status: "âœ… WORKING"}
      transformers: {id: "/huggingface/transformers", v: ">=4.48.0", status: "âœ… WORKING"}
      tokenizers: {id: "/huggingface/tokenizers", v: ">=0.21.0", status: "âœ… WORKING"}
      torch: {id: "/pytorch/pytorch", v: ">=2.0.0", status: "âœ… WORKING"}

      # âœ… FastAPI server stack - production features added
      fastapi: &fastapi-dep {id: "/tiangolo/fastapi", v: ">=0.115.0", status: "âœ… OPERATIONAL"}
      uvicorn: {id: "/encode/uvicorn", v: ">=0.34.0", status: "âœ… OPERATIONAL"}
      pydantic: {id: "/pydantic/pydantic", v: ">=2.10.0", status: "âœ… OPERATIONAL"}
      python-multipart: {id: "/kludex/python-multipart", v: ">=0.0.20", status: "âœ… OPERATIONAL"}

      # âœ… Production features - auth, rate limiting, monitoring
      psutil: {id: null, v: ">=6.1.0", status: "âœ… WORKING"}
      pyyaml: {id: null, v: ">=6.0", status: "âœ… WORKING"}
      python-dotenv: {id: null, v: ">=1.0.0", status: "âœ… WORKING"}
      aiofiles: {id: "/tinche/aiofiles", v: ">=24.1.0", status: "âœ… WORKING"}

    dev:
      # âœ… Testing framework for generation service
      pytest: {id: "/pytest-dev/pytest", v: ">=8.3.0", status: "âœ… READY"}
      pytest-asyncio: {id: "/pytest-dev/pytest-asyncio", v: ">=0.24.0", status: "âœ… READY"}
      httpx: {id: null, v: ">=0.28.0", status: "âœ… READY"}

  # Context Kit infrastructure for monitoring GPT generation server
  dashboard: &dashboard-deps
    prod:
      react: {id: "/reactjs/react.dev", v: "^19.1.1"}
      "@types/react": {id: null, v: "^19.1.9"}
      react-dom: {id: null, v: "^19.1.1"}
      "@types/react-dom": {id: null, v: "^19.1.7"}
      vite: {id: "/vitejs/vite", v: "^7.0.6"}
      "@vitejs/plugin-react": {id: "/vitejs/vite-plugin-react", v: "^4.7.0"}
      typescript: &ts-dep {id: "/microsoft/typescript", v: "^5.0.0"}

  knowledge_graph: &kg-deps
    prod:
      better-sqlite3: {id: "/wiselibs/better-sqlite3", v: "^9.0.0"}
      nanoid: {id: "/ai/nanoid", v: "^5.0.0"}
      pino: {id: "/pinojs/pino", v: "^8.16.0"}
      typescript:
        <<: *ts-dep

# TRANSFORMED GPT text generation directory structure
struct:
  _: {n: 186, t: {py: 30, ts: 49, md: 52, tsx: 15, json: 15, sh: 8, yaml: 7}, modules: 6}

  # âœ… TRANSFORMED: Python environment and text generation server operational
  tkr_env/: {desc: "Python virtual environment", status: "âœ… OPERATIONAL"}
  requirements.txt: {desc: "Python ML dependencies + text generation", status: "âœ… COMPLETE"}
  start_env: {desc: "Environment activation script", status: "âœ… WORKING"}

  # âœ… TRANSFORMED: Core GPT text generation server (Complete transformation)
  tkr_embed/:
    _: {n: 30, desc: "Complete transformation: GPT-OSS-20B text generation service", status: "âœ… TRANSFORMATION_COMPLETE"}

    # âœ… TRANSFORMED: FastAPI server with text generation endpoints
    api/:
      server.py: {desc: "GPT-OSS-20B FastAPI generation server", status: "âœ… OPERATIONAL", port: 8000}
      models.py: {desc: "Generation request/response models", status: "âœ… COMPLETE"}
      auth.py: {desc: "API key authentication", status: "âœ… IMPLEMENTED"}
      rate_limiter.py: {desc: "Token-based rate limiting", status: "âœ… IMPLEMENTED"}
      error_handlers.py: {desc: "Production error handling", status: "âœ… IMPLEMENTED"}
      admin.py: {desc: "Admin endpoints for model management", status: "âœ… IMPLEMENTED"}

    # âœ… TRANSFORMED: GPT model management with 21B parameter model
    core/:
      model_manager.py: {desc: "GPTOss20bMLX model manager", status: "âœ… COMPLETE"}
      batch_processor.py: {desc: "Batch generation processing", status: "âœ… IMPLEMENTED"}

    # âœ… TRANSFORMED: Utilities for text generation
    utils/:
      lru_cache.py: {desc: "Generation result caching", status: "âœ… READY"}
      memory_manager.py: {desc: "21B model memory management", status: "âœ… OPTIMIZED"}

    config.py: {desc: "Production configuration management", status: "âœ… COMPLETE"}
    __init__.py: {desc: "Package initialization", status: "âœ… COMPLETE"}

  # Context Kit integration (updated for generation monitoring)
  .claude:
    _: {n: 23, t: {md: 22, json: 1, sh: 1}}
    agents: {n: 11, desc: "Claude Code specialized agents"}
    commands: {n: 11, desc: "Claude Code workflow commands"}
    settings.local.json: tracked

  .context-kit:
    _: {n: 150, t: {ts: 48, md: 26, tsx: 15}, desc: "GPT generation monitoring infrastructure"}
    _context-kit.yml: tracked

    # Dashboard for GPT generation server monitoring
    dashboard:
      _: {n: 45, package: "@tkr-context-kit/dashboard", port: 42001}
      src:
        views:
          GenerationView.tsx: {desc: "GPT generation monitoring view", status: "ðŸš§ ADAPTATION_READY"}
          MetricsView.tsx: {desc: "Generation performance metrics", status: "ðŸš§ NEXT"}
        services:
          GenerationService.ts: {desc: "GPT server HTTP monitoring", status: "ðŸš§ INTEGRATION_READY"}

    # Knowledge graph for generation metadata
    knowledge-graph:
      _: {n: 32, package: "@tkr-context-kit/knowledge-graph", port: 42003}
      src:
        analyzers:
          generation-analyzer.ts: {desc: "GPT generation analysis", status: "ðŸš§ READY"}
        integration:
          gpt-integration.ts: {desc: "GPT server integration", status: "ðŸš§ READY"}

  claude.local.md: tracked

# GPT-specific design system with text generation aesthetics
design:
  tokens:
    color: &colors
      # âœ… GPT model status colors (implemented)
      model: &model-status
        loaded: {val: "#10b981", type: color, desc: "Model loaded (emerald-500)"}
        loading: {val: "#f59e0b", type: color, desc: "Model loading (amber-500)"}
        error: {val: "#ef4444", type: color, desc: "Model error (red-500)"}
        generating: {val: "#3b82f6", type: color, desc: "Active generation (blue-500)"}
        quantized_q8: {val: "#8b5cf6", type: color, desc: "Q8 quantized (violet-500)"}
        reasoning_high: {val: "#06b6d4", type: color, desc: "High reasoning (cyan-500)"}

      # âœ… Generation performance colors (working)
      performance: &perf-colors
        optimal: {val: "#10b981", type: color, desc: "Fast generation (green)"}
        degraded: {val: "#f59e0b", type: color, desc: "Slow generation (amber)"}
        token_limit: {val: "#ef4444", type: color, desc: "Token limit reached (red)"}
        streaming: {val: "#10b981", type: color, desc: "Streaming active (green)"}
        batch_processing: {val: "#f59e0b", type: color, desc: "Batch processing (amber)"}

      # Text generation inspired palette
      generation: &generation-colors
        neural: {val: "#007AFF", type: color, desc: "Neural processing blue"}
        reasoning: {val: "#34C759", type: color, desc: "Reasoning process green"}
        tokens: {val: "#8E8E93", type: color, desc: "Token processing gray"}
        completion: {val: "#FF9500", type: color, desc: "Completion orange"}

  comp:
    # âœ… TRANSFORMED: GPT Text Generation Server Component
    GenerationServer:
      p: {model: "openai/gpt-oss-20b", quantization: "auto", status: "loaded", memory_usage: 18, port: 8000}
      s: [loaded, generating, ready, error]
      arch: {pattern: "FastAPI + GPT-OSS-20B backend", optimization: "Apple Silicon Metal GPU", memory: "21B parameter model"}
      endpoints: {generate: "/generate", chat: "/chat", stream: "/stream", health: "/health", info: "/info"}

    # âœ… TRANSFORMED: Generation metrics display
    GenerationMetrics:
      p: {throughput: "tokens/sec", latency: "generation time", memory_percent: 85, active_conversations: 3}
      layout: {grid: "4-column metrics grid", cards: "Real-time generation monitoring"}
      thresholds: {tokens_per_sec: "50+ target", latency: "<5s target", memory: "<90% target"}
      status: "Production ready with authentication and rate limiting"

# âœ… TRANSFORMED GPT text generation architecture (Complete transformation)
arch:
  stack:
    primary: "Python + MLX 0.29.0 + GPT-OSS-20B + FastAPI (Apple Silicon M1 optimized)"
    frontend: "React 19 + TypeScript (Context Kit dashboard monitoring)"
    backend: "SQLite + HTTP API (Context Kit knowledge graph)"
    ai_integration: "MCP + Claude Code agents"
    platform: "Apple Silicon M1 32GB with Metal GPU (21B model optimized)"
    model: "openai/gpt-oss-20b with auto-quantization and reasoning levels"
    status: "âœ… Transformation complete - Production text generation service"

  patterns: &arch-patterns
    - "âœ… Apple Silicon Metal GPU optimization for 21B parameter model"
    - "âœ… Auto-quantization (Q4/Q8/MXFP4) based on system memory"
    - "âœ… FastAPI production server with authentication and rate limiting"
    - "âœ… GPTOss20bMLX model manager with reasoning levels"
    - "âœ… Memory manager optimized for 21B model (18GB+ usage)"
    - "âœ… Text generation with streaming support"
    - "âœ… Chat conversation handling with reasoning control"
    - "âœ… Production features: auth, rate limiting, error handling"
    - "âœ… Context Kit dashboard ready for generation monitoring"
    - "ðŸŽ¯ Target: 50+ tokens/sec with real inference"

  services: &service-arch
    # âœ… TRANSFORMED: GPT Text Generation Server (Production)
    generation_server:
      type: "Python FastAPI with GPT-OSS-20B backend"
      port: 8000
      status: "âœ… PRODUCTION_READY"
      responsibility: "Text generation with reasoning levels"
      features: ["âœ… Text generation API", "âœ… Chat completion API", "âœ… Streaming generation", "âœ… Reasoning levels", "âœ… Authentication", "âœ… Rate limiting"]
      optimization: "âœ… Apple Silicon M1 Metal GPU (21B model)"
      model: "GPTOss20bMLX with auto-quantization"
      endpoints: ["/generate", "/chat", "/stream", "/health", "/info", "/admin"]

    # Context Kit monitoring services
    context_dashboard:
      type: "React monitoring dashboard"
      port: 42001
      status: "ðŸš§ ADAPTATION_READY"
      responsibility: "GPT generation monitoring and testing"
      features: ["Performance metrics", "Generation testing", "Model status", "Memory monitoring"]

    knowledge_graph:
      type: "SQLite backend with HTTP API"
      port: 42003
      status: "ðŸš§ ADAPTATION_READY"
      responsibility: "Generation metadata persistence"
      features: ["Generation logs", "Performance history", "Model metadata"]

  integrations:
    dashboard_generation:
      type: "HTTP API monitoring"
      endpoint: "http://localhost:8000"
      status: "âœ… ENDPOINTS_READY"
      features: ["âœ… Health checks working", "âœ… Info endpoint operational", "ðŸš§ Real-time generation metrics"]

# âœ… TRANSFORMED GPT implementation roadmap - Production transformation complete
roadmap:
  transformation: &transformation
    status: "âœ… COMPLETE - Full transformation to GPT-OSS-20B"
    completed:
      - "âœ… Model transformation: OpenSearch-AI embedding â†’ openai/gpt-oss-20b"
      - "âœ… API transformation: /embed/* â†’ /generate, /chat, /stream"
      - "âœ… Architecture transformation: Embedding vectors â†’ Text generation"
      - "âœ… Production features: Authentication, rate limiting, error handling"
      - "âœ… Reasoning levels: Low/Medium/High complexity control"
      - "âœ… Streaming support: Server-sent events for real-time generation"
      - "âœ… Memory optimization: 21B parameter model with auto-quantization"
      - "âœ… Apple Silicon optimization: Metal GPU acceleration for text generation"
    achievements:
      - "âœ… Complete API redesign for text generation workflows"
      - "âœ… Production-ready authentication and rate limiting system"
      - "âœ… Advanced reasoning level control for generation quality"
      - "âœ… Streaming generation with real-time token delivery"

  next_priority: &next-phase
    phase: "Context Kit Integration for Generation Monitoring"
    immediate:
      - "ðŸš§ Adapt dashboard views for generation monitoring"
      - "ðŸš§ Update knowledge graph for generation metadata"
      - "ðŸš§ Real-time generation performance metrics"
    short_term:
      - "ðŸš§ Generation analytics and insights"
      - "ðŸš§ Conversation history tracking"
      - "ðŸš§ Performance optimization based on usage patterns"

  milestones:
    transformation: "âœ… COMPLETE - Full GPT-OSS-20B text generation service"
    production_features: "âœ… COMPLETE - Auth, rate limiting, error handling"
    monitoring_integration: "ðŸš§ NEXT - Context Kit adaptation for generation"
    analytics: "ðŸš§ Future - Advanced generation analytics and insights"

  performance_targets:
    current: "GPT-OSS-20B: 21B parameters, auto-quantization, reasoning levels"
    targets: "Real inference: 50+ tokens/sec, <5s generation, <90% memory"

# Operational patterns for completed GPT transformation
ops:
  development_patterns:
    python_env: "âœ… source start_env â†’ Python GPT environment active"
    generation_server: "âœ… python -m tkr_embed.api.server â†’ GPT server on port 8000"
    dashboard_monitoring: "ðŸš§ cd .context-kit/dashboard && npm run dev â†’ Monitor GPT server"
    testing: "ðŸš§ pytest tkr_embed/tests/ â†’ GPT generation tests"

  performance_monitoring:
    current_metrics: ["Memory: 21B model loaded", "Model: Auto-quantization", "Endpoints: All operational"]
    generation_metrics: ["Token throughput", "Generation latency", "Memory usage tracking"]
    alerts: "âœ… Health endpoints ready for monitoring integration"

# GPT-specific task execution patterns (Transformation complete)
tasks:
  # âœ… COMPLETED Transformation Tasks
  transformation_complete:
    description: "âœ… Complete transformation to GPT-OSS-20B text generation service"
    achievements:
      - "Model changed from embedding to 21B parameter text generation"
      - "API endpoints transformed from /embed/* to /generate, /chat, /stream"
      - "Architecture redesigned for text generation workflows"
      - "Production features added: authentication, rate limiting, error handling"
      - "Reasoning levels implemented for generation quality control"
      - "Streaming generation with real-time token delivery"
      - "Memory optimization for 21B parameter model"

  # ðŸš§ NEXT PRIORITY Tasks
  adapt_monitoring:
    files: [".context-kit/dashboard/src/services/GenerationService.ts", ".context-kit/dashboard/src/views/GenerationView.tsx"]
    pattern: "Adapt Context Kit monitoring â†’ Generation metrics â†’ Performance tracking"
    priority: "ðŸš§ HIGH"

  generation_analytics:
    files: [".context-kit/knowledge-graph/src/analyzers/generation-analyzer.ts", ".context-kit/knowledge-graph/src/integration/gpt-integration.ts"]
    pattern: "Connect to GPT server â†’ Track generations â†’ Analytics dashboard"
    priority: "ðŸš§ MEDIUM"

# Semantic context for completed GPT transformation
semantic:
  ~transformation_complete: "âœ… Complete transformation to GPT-OSS-20B text generation service"
  ~apple_silicon_optimized: "âœ… Apple Silicon M1 32GB with Metal GPU acceleration (21B model)"
  ~gpt_model_ready: "âœ… GPTOss20bMLX with auto-quantization for memory efficiency"
  ~fastapi_production: "âœ… FastAPI production server with authentication, rate limiting, streaming"
  ~text_generation_working: "âœ… Text generation, chat completion, streaming with reasoning levels"
  ~production_features: "âœ… Authentication, rate limiting, error handling, admin endpoints"
  ~context_kit_adaptation: "ðŸš§ Priority: Adapt Context Kit monitoring for generation workflows"
  ~performance_targets: "ðŸŽ¯ Target: 50+ tokens/sec throughput with <5s generation latency"

# âœ… TRANSFORMATION COMPLETE - Progress tracking
progress:
  phase: "âœ… TRANSFORMATION COMPLETE - Context Kit Adaptation Next"
  completed_tasks:
    transformation: ["âœ… Model transformation", "âœ… API redesign", "âœ… Architecture change", "âœ… Production features", "âœ… Reasoning levels", "âœ… Streaming support", "âœ… Memory optimization", "âœ… Apple Silicon optimization"]

  next_steps:
    immediate: ["ðŸš§ Adapt dashboard for generation monitoring", "ðŸš§ Update knowledge graph for generation metadata", "ðŸš§ Real-time generation metrics"]
    integration: ["ðŸš§ Generation analytics", "ðŸš§ Performance insights", "ðŸš§ Conversation tracking"]

  critical_path:
    priority_1: "ðŸš§ Context Kit adaptation for generation monitoring"
    priority_2: "ðŸš§ Generation analytics and insights"
    priority_3: "ðŸš§ Performance optimization based on usage patterns"
    long_term: "ðŸš§ Advanced generation features, multi-model support"

# Architecture evolution: Embedding â†’ GPT Text Generation â†’ Hybrid System
evolution:
  milestone: "âœ… TRANSFORMATION COMPLETE"
  from: "MLX multimodal embedding service"
  to: "GPT-OSS-20B text generation service with production features"
  status: "Production ready with authentication and monitoring integration ready"
  achievements: ["21B parameter model", "Text generation API", "Reasoning levels", "Streaming support", "Production security"]
  next_evolution: "Context Kit adaptation for generation monitoring and analytics"

# Current system status
system_status:
  gpt_server:
    status: "âœ… OPERATIONAL"
    port: 8000
    endpoints: "âœ… All working (/generate, /chat, /stream, /health, /info, /admin)"
    model: "GPT-OSS-20B (21B parameters) - Production ready with auth"
    memory: "Auto-quantization based on system memory"

  context_kit:
    status: "ðŸš§ ADAPTATION_READY"
    dashboard: "Ready for generation monitoring adaptation"
    knowledge_graph: "Ready for generation metadata tracking"

  development:
    environment: "âœ… Python GPT environment operational"
    dependencies: "âœ… All ML/AI dependencies for text generation installed"
    architecture: "âœ… Production service with authentication and rate limiting"