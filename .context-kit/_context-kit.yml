# Project configuration for AI agents - tkr-embed
# MLX multimodal embedding server with Context Kit integration
# Optimized for Apple Silicon and token efficiency
meta:
  kit: tkr-embed
  fmt: 7
  type: mlx-multimodal-embedding-server
  desc: "Apple Silicon optimized multimodal embedding server using MLX and OpenSearch-AI/Ops-MM-embedding-v1-7B"
  ver: "1.0.0"
  author: "Tucker github.com/tuckertucker"
  ts: "2025-09-11"
  status: development
  entry: "start_env"
  stack: &tech-stack "Python + MLX + FastAPI + Context Kit + Apple Silicon + Multimodal"
  model: "OpenSearch-AI/Ops-MM-embedding-v1-7B"
  platform: "Apple Silicon (M1/M2/M3)"
  cmds: ["source start_env", "python -m tkr_embed.server", "cd .context-kit/dashboard && npm run dev"]

# Dependencies with Context7 references - hybrid Python/TypeScript architecture
deps: &deps
  # Python ML/AI stack for embedding server
  py: &py-deps
    prod:
      # MLX ecosystem (Apple Silicon optimized)
      mlx: &mlx-dep {id: "/ml-explore/mlx", v: ">=0.22.0"}
      mlx-lm: {id: "/ml-explore/mlx-lm", v: ">=0.22.0"}
      
      # ML and model hosting
      huggingface-hub: {id: "/huggingface/huggingface_hub", v: ">=0.27.0"}
      transformers: {id: "/huggingface/transformers", v: ">=4.48.0"}
      tokenizers: {id: "/huggingface/tokenizers", v: ">=0.21.0"}
      sentencepiece: {id: "/google/sentencepiece", v: ">=0.2.0"}
      
      # FastAPI server stack
      fastapi: &fastapi-dep {id: "/tiangolo/fastapi", v: ">=0.115.0"}
      uvicorn: {id: "/encode/uvicorn", v: ">=0.34.0"}
      pydantic: {id: "/pydantic/pydantic", v: ">=2.10.0"}
      python-multipart: {id: "/kludex/python-multipart", v: ">=0.0.20"}
      
      # Multimodal processing
      pillow: {id: "/python-pillow/pillow", v: ">=10.4.0"}
      opencv-python-headless: {id: "/opencv/opencv-python", v: ">=4.10.0"}
      numpy: {id: "/numpy/numpy", v: ">=1.26.0"}
      
      # Utilities
      psutil: {id: null, v: ">=6.1.0", desc: "System monitoring"}
      pyyaml: {id: null, v: ">=6.0", desc: "Config management"}
      python-dotenv: {id: null, v: ">=1.0.0", desc: "Environment variables"}
      aiofiles: {id: "/tinche/aiofiles", v: ">=24.1.0"}
      
    dev:
      pytest: {id: "/pytest-dev/pytest", v: ">=8.3.0"}
      pytest-asyncio: {id: "/pytest-dev/pytest-asyncio", v: ">=0.24.0"}
      httpx: {id: null, v: ">=0.28.0", desc: "Testing HTTP client"}

  # Context Kit infrastructure (TypeScript)
  dashboard: &dashboard-deps
    prod:
      # React ecosystem for monitoring dashboard
      react: {id: "/reactjs/react.dev", v: "^19.1.1"}
      "@types/react": {id: null, v: "^19.1.9"}
      react-dom: {id: null, v: "^19.1.1"}
      "@types/react-dom": {id: null, v: "^19.1.7"}
      reactflow: {id: "/xyflow/xyflow", v: "^11.11.4"}
      
      # Build tooling
      vite: {id: "/vitejs/vite", v: "^7.0.6"}
      "@vitejs/plugin-react": {id: "/vitejs/vite-plugin-react", v: "^4.7.0"}
      typescript: &ts-dep {id: "/microsoft/typescript", v: "^5.0.0"}
    dev:
      "@types/node": {id: null, v: "^20.0.0"}
      "@typescript-eslint/eslint-plugin": &ts-eslint {id: "/typescript-eslint/typescript-eslint", v: "^6.0.0"}
      "@typescript-eslint/parser": 
        <<: *ts-eslint
      eslint: {id: "/eslint/eslint", v: "^8.0.0"}

  knowledge_graph: &kg-deps
    prod:
      # Context Kit backend
      better-sqlite3: {id: "/wiselibs/better-sqlite3", v: "^9.0.0"}
      nanoid: {id: "/ai/nanoid", v: "^5.0.0"}
      pino: {id: "/pinojs/pino", v: "^8.16.0"}
      typescript:
        <<: *ts-dep

  # Related MLX ecosystem for future expansion
  mlx_ecosystem:
    mlx-data: {id: "/ml-explore/mlx-data", desc: "MLX data loading"}
    mlx-audio: {id: "/blaizzy/mlx-audio", desc: "Text-to-speech with MLX"}
    mlx-embeddings: {id: "/blaizzy/mlx-embeddings", desc: "Vision/Language embeddings"}
    mlx-vlm: {id: "/blaizzy/mlx-vlm", desc: "Vision Language Models"}
    mlx-swift: {id: "/ml-explore/mlx-swift", desc: "Swift API for MLX"}

# Hybrid directory structure - Python ML server + TypeScript Context Kit
struct:
  _: {n: 174, t: {ts: 49, md: 48, tsx: 15, py: 1, json: 15, sh: 8, yaml: 7}, modules: 4}
  
  # Python environment and ML server
  tkr_env/: {desc: "Python virtual environment", status: untracked}
  requirements.txt: {desc: "Python dependencies", status: untracked}
  start_env: {desc: "Python environment activation script", status: untracked}
  
  # Core Python embedding server (to be implemented)
  tkr_embed/:
    _: {n: 26, desc: "26-task MLX implementation roadmap"}
    server.py: {desc: "FastAPI embedding server"}
    models/:
      loader.py: {desc: "MLX model loading with quantization"}
      embeddings.py: {desc: "Multimodal embedding generation"}
      quantization.py: {desc: "Apple Silicon memory-based quantization"}
    api/:
      endpoints.py: {desc: "FastAPI endpoints for text/image/multimodal"}
      validation.py: {desc: "Request/response validation"}
      middleware.py: {desc: "Auth and rate limiting"}
    processing/:
      text.py: {desc: "Text preprocessing and tokenization"}
      image.py: {desc: "Image processing with PIL/OpenCV"}
      video.py: {desc: "Video frame extraction and pooling"}
      multimodal.py: {desc: "Multimodal fusion logic"}
    utils/:
      cache.py: {desc: "LRU embedding cache"}
      config.py: {desc: "Configuration management"}
      logging.py: {desc: "Structured logging"}
    tests/: {desc: "Unit and integration tests"}
    config/: {desc: "Hardware-specific configurations"}

  # Context Kit integration (existing infrastructure)
  .claude:
    _: {n: 23, t: {md: 22, json: 1, sh: 1}}
    agents: {n: 11, files: [command-writer, design-system, dir-mapper, docs-context7, kg-initial-analyzer, kg-update, meta-agent, port-consistency, project-yaml-builder, project-yaml-update, storybook-maintainer]}
    commands: {n: 11, files: [categorize_errors, commit, context-init, context_prime, create_plan, five, kg_init, minima, project-yaml, update_claude_code]}
    hooks: {files: [hooks.sh]}
    settings.local.json: tracked

  .context-kit:
    _: {n: 150, t: {ts: 48, md: 26, tsx: 15}, desc: "Context Kit infrastructure"}
    _context-kit.yml: tracked
    
    # MLX specifications and implementation plan
    _specs:
      _: {n: 2}
      mlx_architecture_plan.md: tracked
      mlx_implementation_plan.md: tracked
    
    # Unified dashboard for monitoring embedding server
    dashboard:
      _: {n: 45, t: {tsx: 15, ts: 15}, package: "@tkr-context-kit/dashboard", port: 42001}
      src:
        components:
          common: {files: [ErrorMessage.tsx, LoadingSpinner.tsx, FilterControls.tsx]}
          layout: {files: [Header.tsx, Layout.tsx]}
          ui: {files: [Button.tsx, Card.tsx, StatusBadge.tsx]}
        views: {files: [OverviewView.tsx, EmbeddingView.tsx, MetricsView.tsx, LogsView.tsx]}
        services:
          _: {files: [BaseService.ts, ServiceRegistry.ts]}
          EmbeddingService.ts: {desc: "MLX server monitoring"}
        App.tsx: main-app
        main.tsx: entry-point

    # Knowledge graph backend for embedding metadata
    knowledge-graph:
      _: {n: 32, package: "@tkr-context-kit/knowledge-graph", port: 42003}
      src:
        core: {files: [database.ts, knowledge-graph.ts, types.ts]}
        api: {files: [http-server.ts]}
        analyzers: {files: [embedding-analyzer.ts]}
        integration: {files: [mlx-integration.ts]}

    # Logging infrastructure
    logging-client:
      _: {n: 12, package: "@tkr-context-kit/logging-client"}
      src: {files: [logger.ts, formatters.ts]}

    # MCP integration for AI model context
    mcp:
      _: {n: 25}
      src:
        database: {files: [embedding-schema.sql]}
        tools: {files: [embedding-tools.ts]}
        server.ts: mcp-server
    
    scripts: {files: [check-ports.sh, setup-context-kit-mcp]}
    analysis: {files: [docs-context7-output.yml, dir-structure-output.yml, design-system-output.yml]}

  claude.local.md: tracked

# Design system focused on ML/AI monitoring and Apple Silicon aesthetics
design:
  tokens:
    color: &colors
      # Apple Silicon inspired color palette
      primary: &apple-colors
        neural: {val: "#007AFF", type: color, desc: "Neural network blue"}
        metal: {val: "#8E8E93", type: color, desc: "Metal Performance Shaders"}
        silicon: {val: "#34C759", type: color, desc: "Silicon green - Apple M-series"}
        tensor: {val: "#FF9500", type: color, desc: "Tensor operation orange"}
        
      # MLX model status colors  
      model: &model-status
        loaded: {val: "#34C759", type: color, desc: "Model loaded - green"}
        loading: {val: "#FF9500", type: color, desc: "Model loading - orange"}
        error: {val: "#FF3B30", type: color, desc: "Model error - red"}
        quantized: {val: "#007AFF", type: color, desc: "Quantized model - blue"}
        
      # Performance monitoring colors
      performance: &perf-colors
        optimal: {val: "#34C759", type: color, desc: "Optimal performance"}
        degraded: {val: "#FF9500", type: color, desc: "Performance degraded"}
        memory_high: {val: "#FF3B30", type: color, desc: "High memory usage"}
        gpu_active: {val: "#007AFF", type: color, desc: "GPU actively processing"}
        
      # Dashboard UI (inherited from Context Kit)
      bg:
        primary: {val: "#f8fafc", type: color, desc: "Dashboard background"}
        surface: {val: "#ffffff", type: color, desc: "Card surfaces"}
      text:
        primary: {val: "#1e293b", type: color, desc: "Primary text"}
        secondary: {val: "#475569", type: color, desc: "Secondary text"}

    # MLX-specific typography for metrics and code
    typography: &typography
      font_family:
        system: {val: "-apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif", type: fontFamily}
        code: {val: "'SF Mono', 'Monaco', 'Inconsolata', monospace", type: fontFamily, desc: "MLX code and metrics"}
      font_size:
        metric: {val: "1.5rem", type: dimension, desc: "Large metrics display"}
        code: {val: "0.875rem", type: dimension, desc: "Code and API responses"}
        
    # Spacing optimized for ML monitoring layouts
    spacing: &spacing
      xs: {val: "0.25rem", type: dimension}
      sm: {val: "0.5rem", type: dimension}
      md: {val: "1rem", type: dimension}
      lg: {val: "1.5rem", type: dimension}
      xl: {val: "2rem", type: dimension}
      
      # ML dashboard specific spacing
      metric_grid: {val: "1.5rem", type: dimension, desc: "Spacing between metric cards"}
      embedding_viz: {val: "2rem", type: dimension, desc: "Embedding visualization spacing"}

  comp:
    # MLX Embedding Server Components
    EmbeddingServer:
      p: {model: "string", quantization: "Q4|Q8|none", status: "ModelStatus", memory_usage: "number"}
      s: [initializing, loading_model, ready, error, processing]
      arch: {pattern: "FastAPI with MLX backend", optimization: "Apple Silicon Metal GPU", batching: "Dynamic batch sizing"}
      
    MetricsDisplay:
      p: {throughput: "number", latency: "number", memory_percent: "number", gpu_utilization: "number"}
      layout: {grid: "4-column metrics grid", cards: "Performance metric cards"}
      thresholds: {throughput: "150+ tokens/sec", latency: "<100ms", memory: "<50%"}
      
    EmbeddingView:
      p: {service: "EmbeddingService", embeddings: "EmbeddingResult[]"}
      purpose: "Real-time embedding visualization and testing"
      features: {input_testing: "Text/image input forms", visualization: "Embedding vector display", history: "Recent embedding results"}
      
    # Enhanced Context Kit components for ML monitoring
    App:
      p: {embeddingService: "EmbeddingService", contextKit: "ContextKitServices", modelStatus: "ModelStatus"}
      s: [initializing, model_loading, ready, error]
      integration: {mlx_server: "HTTP API monitoring", health_checks: "Model and service health"}
      
    Button:
      p: {variant: "[neural, metal, silicon, tensor]", size: "[sm, md, lg]", loading: boolean}
      variants: {neural: "Blue neural network style", metal: "Metal gray", silicon: "Apple green", tensor: "Tensor orange"}
      
    StatusBadge:
      p: {status: "ModelStatus|ServiceStatus", type: "model|service|performance"}
      variants: {loaded: "Green with checkmark", loading: "Orange with spinner", error: "Red with X", quantized: "Blue with chip icon"}

  patterns:
    # MLX-specific architectural patterns
    mlx_patterns:
      quantization: {val: "Auto-detect system memory for Q4/Q8/none", type: pattern, desc: "Memory-based quantization selection"}
      metal_optimization: {val: "Metal Performance Shaders GPU acceleration", type: pattern, desc: "Apple Silicon optimization"}
      batch_processing: {val: "Dynamic batch sizing based on memory", type: pattern, desc: "Memory-aware batching"}
      embedding_cache: {val: "LRU cache with hash-based lookup", type: pattern, desc: "Performance optimization"}
      
    # API patterns for embedding server
    api_patterns:
      multimodal: {val: "Text + image fusion for unified embeddings", type: pattern, desc: "Multimodal processing"}
      streaming: {val: "Async processing with progress updates", type: pattern, desc: "Real-time feedback"}
      error_recovery: {val: "Graceful degradation with fallback models", type: pattern, desc: "Reliability"}
      
    # Service integration patterns
    service_patterns:
      health_monitoring: {val: "Real-time MLX server health checks", type: pattern, desc: "Service monitoring"}
      dashboard_integration: {val: "Context Kit dashboard for ML monitoring", type: pattern, desc: "Unified monitoring"}
      mcp_integration: {val: "Embedding metadata in knowledge graph", type: pattern, desc: "AI context persistence"}

  a11y:
    compliance: "WCAG 2.1 AA"
    ml_accessibility: {metrics: "Screen reader friendly performance data", visualization: "Alt text for embedding visualizations", controls: "Keyboard navigation for all ML controls"}

# MLX multimodal embedding architecture 
arch:
  stack:
    primary: "Python + MLX + FastAPI (Apple Silicon optimized)"
    frontend: "React 19 + TypeScript (Context Kit dashboard)"
    backend: "SQLite + HTTP API (Context Kit knowledge graph)"
    ai_integration: "MCP + Claude Code agents"
    platform: "Apple Silicon (M1/M2/M3) with Metal GPU"
    model: "OpenSearch-AI/Ops-MM-embedding-v1-7B"

  patterns: &arch-patterns
    - "Apple Silicon Metal GPU optimization with MLX framework"
    - "Memory-adaptive quantization (Q4/Q8/none) based on system RAM"
    - "Multimodal embedding generation (text + image + video)"
    - "FastAPI async server with batch processing"
    - "Context Kit dashboard integration for monitoring"
    - "MCP integration for persistent embedding metadata"
    - "LRU caching with hash-based embedding lookup"
    - "Graceful degradation with fallback processing"
    - "Real-time performance metrics and health monitoring"
    - "26-task implementation roadmap with clear milestones"

  services: &service-arch
    embedding_server:
      type: "Python FastAPI with MLX backend"
      port: 8000
      responsibility: "Multimodal embedding generation"
      features: ["Text embeddings", "Image embeddings", "Multimodal fusion", "Video processing", "Batch processing"]
      optimization: "Apple Silicon Metal GPU acceleration"
      
    context_dashboard:
      type: "React monitoring dashboard"
      port: 42001
      responsibility: "MLX server monitoring and testing"
      features: ["Performance metrics", "Embedding visualization", "Input testing", "Health monitoring"]
      
    knowledge_graph:
      type: "SQLite backend with HTTP API"
      port: 42003
      responsibility: "Embedding metadata and knowledge persistence"
      features: ["Embedding metadata", "Performance history", "Model information"]
      
    mcp_integration:
      type: "Model Context Protocol server"
      responsibility: "AI context and embedding management"
      features: ["Embedding search", "Context persistence", "Claude Code integration"]

  integrations:
    dashboard_embedding:
      type: "HTTP API monitoring"
      endpoint: "http://localhost:8000"
      features: ["Real-time metrics", "Health checks", "Performance monitoring"]
      
    embedding_kg:
      type: "Metadata persistence"
      features: ["Embedding vectors", "Performance metrics", "Model metadata"]
      
    mcp_claude:
      type: "AI agent integration"
      features: ["Embedding-enhanced context", "Performance optimization", "Automated monitoring"]

# MLX implementation roadmap (26 critical path tasks)
roadmap:
  foundation: &foundation
    - "Task 1-3: MLX installation, model loading, quantization setup"
    - "Task 4-7: Core embedding pipeline (text, image, multimodal, batching)"
    - "Task 8-12: FastAPI server with all endpoints"
  
  optimization: &optimization
    - "Task 13-15: Caching, Metal GPU optimization, request batching"
    - "Task 16-19: Configuration, logging, auth, error handling"
    
  testing: &testing
    - "Task 20-23: Unit tests, integration tests, load testing, benchmarks"
    
  deployment: &deployment
    - "Task 24-26: Setup scripts, documentation, packaging"

  milestones:
    mvp: "After task 11 - Full multimodal API"
    optimized: "After task 15 - Production performance"
    production: "After task 19 - Enterprise ready"
    packaged: "After task 26 - Distribution ready"

  performance_targets:
    throughput: "150+ tokens/second"
    latency: "<100ms per request"
    memory: "<50% system RAM usage"
    concurrent: "100+ concurrent requests"
    model_load: "<10 seconds startup"

# Operational patterns for MLX development
ops:
  paths: &key-paths
    "tkr_embed/": "Python MLX embedding server source"
    "tkr_env/": "Python virtual environment"
    "requirements.txt": "Python dependencies"
    ".context-kit/dashboard/": "React monitoring dashboard"
    ".context-kit/_specs/": "MLX implementation specifications"
    
  development_patterns:
    python_env: "source start_env → Activate Python environment"
    embedding_server: "python -m tkr_embed.server → Start MLX server"
    dashboard: "cd .context-kit/dashboard && npm run dev → Monitor dashboard"
    testing: "pytest tkr_embed/tests/ → Run MLX tests"
    
  performance_monitoring:
    metrics: ["Throughput (tokens/sec)", "Latency (ms)", "Memory usage (%)", "GPU utilization (%)"]
    thresholds: {throughput: 150, latency: 100, memory: 50, gpu: 80}
    alerts: "Dashboard shows performance degradation warnings"

# MLX-specific task execution patterns
tasks:
  develop_embedding_server:
    files: ["tkr_embed/"]
    pattern: "source start_env → Edit Python → Test with sample inputs → Monitor performance"
    
  implement_multimodal:
    files: ["tkr_embed/processing/multimodal.py"]
    pattern: "Text + image fusion → Generate unified embeddings → Validate 1024 dimensions"
    
  optimize_performance:
    files: ["tkr_embed/utils/", "tkr_embed/models/quantization.py"]
    pattern: "Metal GPU optimization → Quantization tuning → Batch size optimization"
    
  test_embedding_pipeline:
    files: ["tkr_embed/tests/"]
    pattern: "Unit tests → Integration tests → Load testing → Performance benchmarks"
    
  monitor_with_dashboard:
    files: [".context-kit/dashboard/src/views/EmbeddingView.tsx"]
    pattern: "Monitor MLX server → View performance metrics → Test embeddings"

# Semantic context for MLX multimodal embedding server
semantic:
  ~mlx_multimodal: "Apple Silicon optimized multimodal embedding server using MLX framework"
  ~ops_mm_embedding: "OpenSearch-AI/Ops-MM-embedding-v1-7B model with quantization support"
  ~apple_silicon: "M1/M2/M3 chip optimization with Metal Performance Shaders GPU acceleration"
  ~multimodal_fusion: "Text + image + video unified embedding generation with 1024 dimensions"
  ~memory_adaptive: "Automatic quantization (Q4/Q8/none) based on available system memory"
  ~fastapi_async: "Async FastAPI server with batch processing and concurrent request handling"
  ~context_kit_integration: "Context Kit dashboard and knowledge graph for MLX monitoring"
  ~performance_targets: "150+ tokens/sec, <100ms latency, <50% memory, 100+ concurrent users"
  ~implementation_roadmap: "26-task development plan with foundation, optimization, testing, deployment phases"

# Implementation progress tracking
progress:
  phase: "Foundation - Task Planning Complete"
  completed: ["Project structure", "Dependencies analysis", "Implementation plan", "Context Kit integration"]
  next_steps: ["MLX installation verification", "Model loading implementation", "Quantization logic", "Basic embedding pipeline"]
  
  critical_path:
    immediate: "Tasks 1-3 (MLX setup, model loading, quantization)"
    short_term: "Tasks 4-8 (embedding pipeline, FastAPI server)" 
    medium_term: "Tasks 9-15 (API endpoints, optimization)"
    long_term: "Tasks 16-26 (production features, testing, deployment)"

# Architecture evolution from Context Kit to MLX focus
evolution:
  from: "TypeScript Context Kit development toolkit"
  to: "Python MLX multimodal embedding server with Context Kit monitoring"
  approach: "Hybrid architecture - Python ML backend + TypeScript dashboard frontend"
  benefits: ["Apple Silicon optimization", "Multimodal capabilities", "Production monitoring", "AI agent integration"]
  maintained: ["Context Kit infrastructure", "Service monitoring", "MCP integration", "Agent workflows"]