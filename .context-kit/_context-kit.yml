# Project configuration for AI agents - tkr-embed
# MLX multimodal embedding server with Context Kit monitoring - Foundation Complete
# Generated from completed MLX foundation analysis - optimized for token efficiency
meta:
  kit: tkr-embed
  fmt: 8
  type: mlx-multimodal-embedding-server
  desc: "COMPLETED FOUNDATION: Apple Silicon MLX embedding server with multimodal processing and Context Kit monitoring"
  ver: "1.1.0"
  author: "Tucker github.com/tuckertucker"
  ts: "2025-09-11"
  status: foundation-complete
  entry: "source start_env"
  stack: &tech-stack "Python + MLX 0.29.0 + FastAPI + Context Kit + Apple Silicon M1 + Multimodal Processing"
  model: "OpenSearch-AI/Ops-MM-embedding-v1-7B (Q8_0 quantized)"
  platform: "Apple Silicon M1 32GB with Metal GPU optimization"
  cmds: ["source start_env", "python -m tkr_embed.api.server", "cd .context-kit/dashboard && npm run dev"]

# Dependencies with Context7 references - COMPLETED MLX foundation
deps: &deps
  # COMPLETED: Python ML/AI stack for embedding server
  py: &py-deps
    prod:
      # âœ… MLX ecosystem installed and tested
      mlx: &mlx-dep {id: "/ml-explore/mlx", v: "0.29.0", status: "âœ… INSTALLED"}
      mlx-lm: {id: "/ml-explore/mlx-lm", v: ">=0.22.0", status: "âœ… INSTALLED"}
      
      # âœ… ML and model hosting - working
      huggingface-hub: {id: "/huggingface/huggingface_hub", v: ">=0.27.0", status: "âœ… WORKING"}
      transformers: {id: "/huggingface/transformers", v: ">=4.48.0", status: "âœ… WORKING"}
      tokenizers: {id: "/huggingface/tokenizers", v: ">=0.21.0", status: "âœ… WORKING"}
      sentencepiece: {id: "/google/sentencepiece", v: ">=0.2.0", status: "âœ… WORKING"}
      
      # âœ… FastAPI server stack - fully operational
      fastapi: &fastapi-dep {id: "/tiangolo/fastapi", v: ">=0.115.0", status: "âœ… OPERATIONAL"}
      uvicorn: {id: "/encode/uvicorn", v: ">=0.34.0", status: "âœ… OPERATIONAL"}
      pydantic: {id: "/pydantic/pydantic", v: ">=2.10.0", status: "âœ… OPERATIONAL"}
      python-multipart: {id: "/kludex/python-multipart", v: ">=0.0.20", status: "âœ… OPERATIONAL"}
      
      # âœ… Multimodal processing - ready for real inference
      pillow: {id: "/python-pillow/pillow", v: ">=10.4.0", status: "âœ… READY"}
      opencv-python-headless: {id: "/opencv/opencv-python", v: ">=4.10.0", status: "âœ… READY"}
      numpy: {id: "/numpy/numpy", v: ">=1.26.0", status: "âœ… READY"}
      
      # âœ… Utilities - tested and working
      psutil: {id: null, v: ">=6.1.0", status: "âœ… WORKING"}
      pyyaml: {id: null, v: ">=6.0", status: "âœ… WORKING"}
      python-dotenv: {id: null, v: ">=1.0.0", status: "âœ… WORKING"}
      aiofiles: {id: "/tinche/aiofiles", v: ">=24.1.0", status: "âœ… WORKING"}
      
    dev:
      # âœ… Testing framework ready
      pytest: {id: "/pytest-dev/pytest", v: ">=8.3.0", status: "âœ… READY"}
      pytest-asyncio: {id: "/pytest-dev/pytest-asyncio", v: ">=0.24.0", status: "âœ… READY"}
      httpx: {id: null, v: ">=0.28.0", status: "âœ… READY"}

  # Context Kit infrastructure for monitoring MLX server
  dashboard: &dashboard-deps
    prod:
      react: {id: "/reactjs/react.dev", v: "^19.1.1"}
      "@types/react": {id: null, v: "^19.1.9"}
      react-dom: {id: null, v: "^19.1.1"}
      "@types/react-dom": {id: null, v: "^19.1.7"}
      vite: {id: "/vitejs/vite", v: "^7.0.6"}
      "@vitejs/plugin-react": {id: "/vitejs/vite-plugin-react", v: "^4.7.0"}
      typescript: &ts-dep {id: "/microsoft/typescript", v: "^5.0.0"}

  knowledge_graph: &kg-deps
    prod:
      better-sqlite3: {id: "/wiselibs/better-sqlite3", v: "^9.0.0"}
      nanoid: {id: "/ai/nanoid", v: "^5.0.0"}
      pino: {id: "/pinojs/pino", v: "^8.16.0"}
      typescript:
        <<: *ts-dep

# COMPLETED MLX foundation directory structure 
struct:
  _: {n: 182, t: {py: 26, ts: 49, md: 50, tsx: 15, json: 15, sh: 8, yaml: 7}, modules: 5}
  
  # âœ… COMPLETED: Python environment and ML server operational
  tkr_env/: {desc: "Python virtual environment", status: "âœ… OPERATIONAL"}
  requirements.txt: {desc: "Python ML dependencies", status: "âœ… COMPLETE"}
  start_env: {desc: "Environment activation script", status: "âœ… WORKING"}
  
  # âœ… COMPLETED: Core MLX embedding server (Foundation phase done)
  tkr_embed/:
    _: {n: 26, desc: "Foundation phase complete: Tasks 1-12 of 27", status: "âœ… FOUNDATION_COMPLETE"}
    
    # âœ… COMPLETED: FastAPI server with all endpoints
    api/:
      server.py: {desc: "FastAPI embedding server", status: "âœ… OPERATIONAL", port: 8000}
      endpoints.py: {desc: "Multimodal API endpoints", status: "âœ… ALL_ENDPOINTS_WORKING"}
      validation.py: {desc: "Request/response validation", status: "âœ… WORKING"}
      middleware.py: {desc: "CORS and error handling", status: "âœ… WORKING"}
    
    # âœ… COMPLETED: MLX model management with auto-quantization
    models/:
      manager.py: {desc: "OpsMMEmbeddingMLX model manager", status: "âœ… COMPLETE"}
      embeddings.py: {desc: "Multimodal embedding generation", status: "âœ… MOCK_WORKING"}
      quantization.py: {desc: "Q8_0 auto-quantization", status: "âœ… IMPLEMENTED"}
      memory.py: {desc: "Metal GPU memory manager (24GB)", status: "âœ… OPTIMIZED"}
    
    # âœ… COMPLETED: Multimodal processing pipeline
    processing/:
      text.py: {desc: "Text preprocessing", status: "âœ… READY"}
      image.py: {desc: "Image processing with PIL", status: "âœ… READY"}
      multimodal.py: {desc: "Multimodal fusion", status: "âœ… MOCK_COMPLETE"}
    
    # âœ… COMPLETED: Utilities and configuration
    utils/:
      cache.py: {desc: "LRU embedding cache", status: "âœ… READY"}
      config.py: {desc: "Hardware-aware config", status: "âœ… WORKING"}
      logging.py: {desc: "Structured logging", status: "âœ… WORKING"}
      validation.py: {desc: "Input validation", status: "âœ… WORKING"}
    
    tests/: {desc: "Unit and integration tests", status: "ðŸš§ NEXT_PRIORITY"}

  # Context Kit integration (existing infrastructure)
  .claude:
    _: {n: 23, t: {md: 22, json: 1, sh: 1}}
    agents: {n: 11, desc: "Claude Code specialized agents"}
    commands: {n: 11, desc: "Claude Code workflow commands"}
    settings.local.json: tracked

  .context-kit:
    _: {n: 150, t: {ts: 48, md: 26, tsx: 15}, desc: "MLX monitoring infrastructure"}
    _context-kit.yml: tracked
    
    # Dashboard for MLX server monitoring
    dashboard:
      _: {n: 45, package: "@tkr-context-kit/dashboard", port: 42001}
      src:
        views: 
          EmbeddingView.tsx: {desc: "MLX server monitoring view", status: "ðŸš§ ENHANCEMENT_READY"}
          MetricsView.tsx: {desc: "Performance metrics display", status: "ðŸš§ NEXT"}
        services:
          EmbeddingService.ts: {desc: "MLX server HTTP monitoring", status: "ðŸš§ INTEGRATION_READY"}
    
    # Knowledge graph for embedding metadata
    knowledge-graph:
      _: {n: 32, package: "@tkr-context-kit/knowledge-graph", port: 42003}
      src:
        analyzers: 
          embedding-analyzer.ts: {desc: "MLX embedding analysis", status: "ðŸš§ READY"}
        integration:
          mlx-integration.ts: {desc: "MLX server integration", status: "ðŸš§ READY"}

  claude.local.md: tracked

# MLX-specific design system with Apple Silicon aesthetics
design:
  tokens:
    color: &colors
      # âœ… MLX model status colors (implemented)
      model: &model-status
        loaded: {val: "#10b981", type: color, desc: "Model loaded (emerald-500)"}
        loading: {val: "#f59e0b", type: color, desc: "Model loading (amber-500)"}
        error: {val: "#ef4444", type: color, desc: "Model error (red-500)"}
        quantized_q8: {val: "#3b82f6", type: color, desc: "Q8_0 quantized (blue-500)"}
        quantized_q4: {val: "#8b5cf6", type: color, desc: "Q4 quantized (violet-500)"}
        metal_gpu: {val: "#06b6d4", type: color, desc: "Metal GPU active (cyan-500)"}
      
      # âœ… Performance monitoring colors (working)
      performance: &perf-colors
        optimal: {val: "#10b981", type: color, desc: "150+ tokens/sec (green)"}
        degraded: {val: "#f59e0b", type: color, desc: "Performance degraded (amber)"}
        memory_high: {val: "#ef4444", type: color, desc: ">50% memory usage (red)"}
        latency_good: {val: "#10b981", type: color, desc: "<100ms latency (green)"}
        latency_slow: {val: "#f59e0b", type: color, desc: ">100ms latency (amber)"}
      
      # Apple Silicon inspired palette
      apple: &apple-colors
        neural: {val: "#007AFF", type: color, desc: "Neural network blue"}
        silicon: {val: "#34C759", type: color, desc: "Apple Silicon green"}
        metal: {val: "#8E8E93", type: color, desc: "Metal Performance Shaders"}
        tensor: {val: "#FF9500", type: color, desc: "Tensor operation orange"}

  comp:
    # âœ… COMPLETED: MLX Embedding Server Component
    EmbeddingServer:
      p: {model: "OpenSearch-AI/Ops-MM-embedding-v1-7B", quantization: "Q8_0", status: "loaded", memory_usage: 24, port: 8000}
      s: [loaded, processing, ready, error]
      arch: {pattern: "FastAPI + MLX backend", optimization: "Apple Silicon Metal GPU", memory: "24GB GPU allocation"}
      endpoints: {text: "/embed/text", image: "/embed/image", multimodal: "/embed/multimodal", health: "/health", info: "/info"}
      
    # âœ… COMPLETED: Performance metrics display
    MetricsDisplay:
      p: {throughput: "mock: 1024-dim vectors", latency: "~50ms mock", memory_percent: 75, gpu_utilization: 85}
      layout: {grid: "4-column metrics grid", cards: "Real-time performance monitoring"}
      thresholds: {throughput: "150+ tokens/sec target", latency: "<100ms target", memory: "<50% target"}
      status: "Foundation complete, ready for real model loading"

# âœ… COMPLETED MLX multimodal embedding architecture (Foundation phase)
arch:
  stack:
    primary: "Python + MLX 0.29.0 + FastAPI (Apple Silicon M1 optimized)"
    frontend: "React 19 + TypeScript (Context Kit dashboard monitoring)"
    backend: "SQLite + HTTP API (Context Kit knowledge graph)"
    ai_integration: "MCP + Claude Code agents"
    platform: "Apple Silicon M1 32GB with Metal GPU (24GB allocated)"
    model: "OpenSearch-AI/Ops-MM-embedding-v1-7B with Q8_0 quantization"
    status: "âœ… Foundation complete - 12 of 27 tasks done"

  patterns: &arch-patterns
    - "âœ… Apple Silicon Metal GPU optimization with MLX 0.29.0"
    - "âœ… Memory-adaptive quantization (Q8_0) for 32GB M1 system"  
    - "âœ… FastAPI async server with all multimodal endpoints working"
    - "âœ… OpsMMEmbeddingMLX model manager with auto-quantization"
    - "âœ… Memory manager with 24GB Metal GPU allocation"
    - "âœ… Mock embedding generation (1024-dimension vectors)"
    - "âœ… Complete error handling and async processing"
    - "âœ… Context Kit dashboard integration architecture ready"
    - "ðŸš§ Next: Load real Ops-MM-embedding-v1-7B model (~15GB)"
    - "ðŸŽ¯ Target: 150+ tokens/sec with real inference"

  services: &service-arch
    # âœ… COMPLETED: MLX Embedding Server (Foundation)
    embedding_server:
      type: "Python FastAPI with MLX backend"
      port: 8000
      status: "âœ… FOUNDATION_COMPLETE"
      responsibility: "Multimodal embedding generation"
      features: ["âœ… Text embeddings API", "âœ… Image embeddings API", "âœ… Multimodal fusion API", "âœ… Health monitoring", "âœ… Info endpoints"]
      optimization: "âœ… Apple Silicon M1 Metal GPU (24GB allocated)"
      model: "OpsMMEmbeddingMLX with Q8_0 quantization"
      next: "Load real OpenSearch-AI/Ops-MM-embedding-v1-7B model"
      
    # Context Kit monitoring services
    context_dashboard:
      type: "React monitoring dashboard"
      port: 42001
      status: "ðŸš§ INTEGRATION_READY"
      responsibility: "MLX server monitoring and testing"
      features: ["Performance metrics", "Embedding testing", "Model status", "Memory monitoring"]
      
    knowledge_graph:
      type: "SQLite backend with HTTP API"
      port: 42003
      status: "ðŸš§ INTEGRATION_READY"
      responsibility: "Embedding metadata persistence"
      features: ["Embedding vectors", "Performance history", "Model metadata"]

  integrations:
    dashboard_embedding:
      type: "HTTP API monitoring"
      endpoint: "http://localhost:8000"
      status: "âœ… ENDPOINTS_READY"
      features: ["âœ… Health checks working", "âœ… Info endpoint operational", "ðŸš§ Real-time metrics integration"]

# âœ… COMPLETED MLX implementation roadmap - Foundation phase done
roadmap:
  foundation: &foundation
    status: "âœ… COMPLETE (Tasks 1-12)"
    completed:
      - "âœ… Task 1-3: MLX 0.29.0 installation and testing on 32GB Apple Silicon M1"
      - "âœ… Task 4-6: OpsMMEmbeddingMLX model manager with auto-quantization (Q8_0)"
      - "âœ… Task 7-9: Memory manager with Metal GPU optimization (24GB allocated)"
      - "âœ… Task 10-12: Complete FastAPI server with all endpoints (/embed/text, /embed/image, /embed/multimodal)"
    achievements:
      - "âœ… Mock embedding generation working (1024-dimension vectors)"
      - "âœ… Error handling, validation, and async processing complete"
      - "âœ… Apple Silicon optimization with memory-aware quantization"
      - "âœ… Context Kit integration architecture established"
  
  next_priority: &next-phase
    phase: "Real Model Loading (Tasks 13-15)"
    immediate:
      - "ðŸŽ¯ Load actual OpenSearch-AI/Ops-MM-embedding-v1-7B model (~15GB)"
      - "ðŸŽ¯ Replace mock embeddings with real MLX inference"
      - "ðŸŽ¯ Benchmark performance (target: 150+ tokens/sec)"
    short_term:
      - "ðŸš§ LRU caching implementation"
      - "ðŸš§ Batch processing optimization"
      - "ðŸš§ Context Kit dashboard integration"
  
  milestones:
    foundation: "âœ… COMPLETE - Full multimodal API with mock embeddings"
    real_inference: "ðŸŽ¯ NEXT - Real model loading and inference"
    optimized: "ðŸš§ After task 19 - Production performance"
    production: "ðŸš§ After task 23 - Enterprise ready"

  performance_targets:
    current: "Mock embeddings: 1024-dim vectors, ~50ms latency"
    targets: "Real inference: 150+ tokens/sec, <100ms, <50% memory"

# Operational patterns for completed MLX foundation
ops:
  development_patterns:
    python_env: "âœ… source start_env â†’ Python MLX environment active"
    embedding_server: "âœ… python -m tkr_embed.api.server â†’ MLX server on port 8000"
    dashboard_monitoring: "ðŸš§ cd .context-kit/dashboard && npm run dev â†’ Monitor MLX server"
    testing: "ðŸš§ pytest tkr_embed/tests/ â†’ MLX foundation tests"
    
  performance_monitoring:
    current_metrics: ["Memory: 24GB GPU allocated", "Model: Q8_0 quantization", "Endpoints: All operational"]
    next_metrics: ["Real inference throughput", "Actual latency measurements", "GPU utilization tracking"]
    alerts: "âœ… Health endpoints ready for monitoring integration"

# MLX-specific task execution patterns (Foundation complete)
tasks:
  # âœ… COMPLETED Foundation Tasks
  foundation_complete:
    description: "âœ… MLX foundation phase complete (Tasks 1-12 of 27)"
    achievements:
      - "MLX 0.29.0 installed and tested on Apple Silicon M1 32GB"
      - "OpsMMEmbeddingMLX model manager with Q8_0 auto-quantization"
      - "Memory manager with Metal GPU optimization (24GB allocated)"
      - "Complete FastAPI server with multimodal endpoints operational"
      - "Mock embedding generation for testing (1024-dimension vectors)"
      - "Error handling, validation, and async processing complete"
    
  # ðŸŽ¯ NEXT PRIORITY Tasks  
  load_real_model:
    files: ["tkr_embed/models/manager.py", "tkr_embed/models/embeddings.py"]
    pattern: "Load OpenSearch-AI/Ops-MM-embedding-v1-7B â†’ Replace mock inference â†’ Benchmark performance"
    priority: "ðŸŽ¯ IMMEDIATE"
    
  integrate_dashboard:
    files: [".context-kit/dashboard/src/services/EmbeddingService.ts", ".context-kit/dashboard/src/views/EmbeddingView.tsx"]
    pattern: "Connect to MLX server â†’ Monitor performance â†’ Test embeddings interface"
    priority: "ðŸš§ HIGH"

# Semantic context for completed MLX foundation
semantic:
  ~foundation_complete: "âœ… MLX foundation phase complete: Server operational with mock embeddings"
  ~apple_silicon_optimized: "âœ… Apple Silicon M1 32GB with Metal GPU acceleration (24GB allocated)"
  ~model_manager_ready: "âœ… OpsMMEmbeddingMLX with Q8_0 quantization for memory efficiency"
  ~fastapi_operational: "âœ… FastAPI server with all multimodal endpoints working (/embed/text, /embed/image, /embed/multimodal)"
  ~mock_embeddings_working: "âœ… 1024-dimension vector generation for testing and development"
  ~context_kit_ready: "âœ… Context Kit monitoring architecture prepared for MLX integration"
  ~next_real_inference: "ðŸŽ¯ Priority: Load actual OpenSearch-AI/Ops-MM-embedding-v1-7B model for real inference"
  ~performance_targets: "ðŸŽ¯ Target: 150+ tokens/sec throughput with <100ms latency and <50% memory usage"

# âœ… FOUNDATION PHASE COMPLETE - Progress tracking
progress:
  phase: "âœ… FOUNDATION COMPLETE - Real Model Loading Next"
  completed_tasks:
    foundation: ["âœ… MLX installation & testing", "âœ… Model manager implementation", "âœ… Memory optimization", "âœ… FastAPI server complete", "âœ… Mock embedding generation", "âœ… Error handling & validation", "âœ… Async processing", "âœ… Context Kit integration ready"]
    
  next_steps:
    immediate: ["ðŸŽ¯ Load OpenSearch-AI/Ops-MM-embedding-v1-7B model", "ðŸŽ¯ Replace mock with real inference", "ðŸŽ¯ Performance benchmarking"]
    integration: ["ðŸš§ Dashboard monitoring", "ðŸš§ Knowledge graph persistence", "ðŸš§ Real-time metrics"]
    
  critical_path:
    priority_1: "ðŸŽ¯ Real model loading (Tasks 13-15)"
    priority_2: "ðŸš§ Performance optimization (Tasks 16-19)" 
    priority_3: "ðŸš§ Context Kit integration (Parallel development)"
    long_term: "ðŸš§ Production features, testing, deployment (Tasks 20-27)"

# Architecture evolution: Context Kit â†’ MLX Foundation â†’ Hybrid System
evolution:
  milestone: "âœ… FOUNDATION PHASE COMPLETE"
  from: "TypeScript Context Kit development toolkit"
  to: "Hybrid MLX Python backend + Context Kit monitoring frontend"
  status: "Foundation operational, real inference next priority"
  achievements: ["Apple Silicon optimization", "Multimodal API architecture", "Memory management", "Service monitoring ready"]
  next_evolution: "Real model inference with performance monitoring integration"

# Current system status
system_status:
  mlx_server:
    status: "âœ… OPERATIONAL"
    port: 8000
    endpoints: "âœ… All working (/embed/text, /embed/image, /embed/multimodal, /health, /info)"
    model: "Mock embeddings (1024-dim) - Ready for real model"
    memory: "24GB Metal GPU allocated"
    
  context_kit:
    status: "ðŸš§ INTEGRATION_READY"
    dashboard: "Ready for MLX monitoring"
    knowledge_graph: "Ready for embedding metadata"
    
  development:
    environment: "âœ… Python MLX environment operational"
    dependencies: "âœ… All ML/AI dependencies installed and tested"
    architecture: "âœ… Service-oriented with health monitoring ready"